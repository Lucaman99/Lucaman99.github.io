\documentclass[10pt, oneside]{article} 
\usepackage{amsmath, amsthm, amssymb, wasysym, verbatim, bbm, color, graphics, geometry, hyperref, biblatex}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{tcolorbox}

\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	urlcolor=blue
}

\addbibresource{ref.bib}

\geometry{tmargin=.75in, bmargin=.75in, lmargin=1.25in, rmargin = 1.25in}
\setlength\parindent{0pt}

\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\tcbuselibrary{theorems}

    \newcommand{\R}{\mathbb{R}}
    \newcommand{\C}{\mathbb{C}}
    \newcommand{\Z}{\mathbb{Z}}
    \newcommand{\N}{\mathbb{N}}
    \newcommand{\Q}{\mathbb{Q}}
    \newcommand{\Cdot}{\boldsymbol{\cdot}}

    \newtheorem{thm}{Theorem}
    \newtheorem{defn}{Definition}
    \newtheorem{conv}{Convention}
    \newtheorem{rem}{Remark}
    \newtheorem{lem}{Lemma}
    \newtheorem{cor}{Corollary}
    \newtheorem{prop}{Proposition}

    \newcommand{\tr}{\mathrm{Tr}}
    \newcommand{\pow}{\mathcal{P}}
     \newcommand{\Null}{\text{null}}


    \title{Axler Algebra Notes, Problems and Solutions}
    \author{Jack Ceroni}
    \date{}

    \begin{document}

    \maketitle
    \tableofcontents

    \vspace{.25in}

    \newpage

    \section{Section 3B}

    \begin{problem}{3.12}

      Suppose that $V$ is finite-dimensional and $T \in \mathcal{L}(V, \ W)$. Prove that there exists a subspace
      $U$ of $V$ such that $U \cap \text{null} \ T = \{0\}$ and $\text{range} \ T = \{Tu : u \in U\}$.

      \end{problem}

      \begin{proof}

      Let us consider a basis $B$ of $\Null \ T$. We then choose some basis $B'$ of $V$, which, by rank-nullity theorem, will
      have cardinality greater than or equal to $B$. We use $B$ to extend $B'$ to a basis $C$ of $V$ (which we can do, as each $B'$
      is linearly independent).
      \newline

      Let $U = \text{span}(C - B')$ (linear combinations of
      the elements in the new basis that are not in $B'$). We assert that this is the $U$ that satisfies these
      conditions.
      \newline

      Firstly, it is clear that $U$ and $\text{null} \ T$ contain the zero vector. In addition, if there were some non-zero vector $v$
      in $U$ and $\text{null} \ T$, this would imply that there exist coefficients such that:

      $$v = a_1 u_1 + \ \cdots \ + a_n u_n = b_1 v_1 + \ \cdots \ + b_m v_m$$

      where $u_i \in U$ and $v_i \in B'$. We know that $U \cup B'$ forms a basis for $V$, so the above equation implies that:

      $$a_j u_j = b_1 v_1 + \ \cdots \ + b_m v_m -  a_1 u_1 + \ \cdots \ + a_{j - 1} v_{j - 1} - a_{j + 1} v_{j + 1} + \ \cdots \ + a_n u_n$$

      where we know that at least one $a_i$ (namely $a_j$) is non-zero, and at least one $b_i$ is non-zero to conclude that the existence of
      $v$ violates the linear independence of $U \cup B'$.
      \newline

      Clearly, $\{Tu : u \in U\} \subset \text{range} \ T$. In addition, we pick some $T(x) \in \text{range} \ T$. We have:

      $$x = a_1 u_1 + \ \cdots \ + a_n u_n + b_1 v_1 + \ \cdots \ + b_m v_m$$

      as $U \cup B'$ is a basis for $V$. We then get:

      $$T(x) =  T(a_1 u_1 + \ \cdots \ + a_n u_n) + T(b_1 v_1 + \ \cdots \ + b_m v_m) = T(a_1 u_1 + \ \cdots \ + a_n u_n) = T(u)$$

      where $u \in U$. Thus, $\text{range} \ T \subset \{Tu : u \in U\}$. We have inclusion both ways, so $\{Tu : u \in U\} = \text{range} \ T$.
      This completes the proof.

      \end{proof}

      \begin{problem}{3.19}

        Suppose that $V$ and $W$ are finite dimensional and $U$ is a subspace of $V$. Prove that there exists $T \in \mathcal{L}(V, \ W)$ such that
        $\Null T = U$ if and only if $\dim U \geq \dim V - \dim W$.

      \end{problem}

      \begin{proof}

        First, assume that exists such a $T$. From rank-nullity theorem, we have:

        $$\dim V = \dim \text{range} \ T + \dim \Null T = \dim \text{range} \ T + \dim U \leq \dim W + \dim U$$

        which clearly implies that $\dim U \geq \dim V - \dim W$. Conversely, assume that $\dim U \geq \dim V - \dim W$.
        Consider the basis $u_1, \ ..., \ u_n$ of $U$. We extend this to a basis for $V$ by adding vectors $v_1, \ ..., \ v_m$.
        \newline

        We define $T$ to be the map that takes each $u_k$ to $0$. We define a basis for $W$, which we label $w_1, \ ..., \ w_r$.
        We know that $\dim W \geq \dim V - \dim U$, which is equal to the number of vectors $v_k$. Thus, we are able to assign
        each $v_k$ to some vector $w_s$ of the basis for $W$.
        \newline

        We have assigned values to each basis vector of $V$, which means that $T$ is linear. In addition, it is clear that $\Null T = U$.

      \end{proof}

    \begin{problem}{3.26}
      Suppose $D \in \mathcal{L}(\mathcal{P}(\mathbb{R}), \ \mathcal{P}(\mathbb{R}))$ is such that $\deg \ Dp = (\deg \ p) - 1$ for
      every non-constant polynomial $p \in \pow(\R)$. Prove that $D$ is surjective.
    \end{problem}

    \begin{proof}

      Consider some $p \in \pow(\R)$ such that the degree of $p$ is $n$. Consider the subset $\{x^{n + 1}, \ x^n, \ ..., \ x\}$ of $\pow(\R)$.
      We map each of these terms under $D$ to get the set $B = \{D(x^{n + 1}, \ D(x^n), \ ..., \ D(x)\}$.
      \newline

      The $k$-th elements of this list will be a polynomial of degree $n + 1 - k$. It is easy to check that such a list is linearly
      independent: we complete the redundancy-removal procedure, starting at $D(x)$, noting that for each $D(x^k)$, we cannot write
      $D(x^k)$ as a sum of the polynomials $\{D(x^{k - 1}, \ ..., \ D(x)\}$ as $D(x^k)$ contains a term of degree $n + 1 - k$, which
      none of the other elements posses.
      \newline

      It follows that the elements of $B$ are linearly independent. Let us consider the subspace $V_n \subset \pow(\R)$ of all polynomials
      of degree $n$. Clearly, such a space will have degree $n + 1$. It is also clear that each element of $B$ is in $V_n$. Thus, $B$ is a
      linearly independent list of length $n + 1$ contained in $V_n$. It follows that $B$ is a basis for $V_n$.
      \newline

      Thus, for the $p$ that we considered initially, we can write:

      $$p = c_1 D(x) + \ \cdots \ + c_{n + 1} D(x^{n + 1}) = D(c_1 x + \ \cdots \ c_{n + 1} x^{n + 1})$$

      Therefore, $p$ can be written asd the image of some element of $\pow(\R)$ and the map $D$ is surjective.

    \end{proof}

    \begin{problem}{3.29}
      Suppose $\phi \in \mathcal{L}(V, \ \mathbb{F})$. Suppose that $u \in V$ is not in $\text{null} \ \phi$. Prove that:

      $$V = \text{null} \ \phi \oplus \{au \ : \ a \in \mathbb{F}\}$$
    \end{problem}

    \begin{proof}

      In the case that $\phi$ is the trivial map, the null space of $\phi$ is all $V$ and the theorem is proved.
      \newline

      In the case that $\phi$ is not the trivial map, we know from rank-nullity theorem that:

      $$\dim \ V = \dim \ \text{null} \ \phi + \dim \ \text{range} \ \phi$$

      However, it is clear that $\text{range} \ \phi = \mathbb{F}$, so $\dim \ \text{range} \ \phi = \dim \ \mathbb{F} = 1$. This
      implies that:

      $$\dim \ V - \dim \ \text{null} \ \phi = 1$$

      Now, we know that given some $V$, and a subspace $U$ of $V$, there exists some $U'$ such that $V = U \oplus U'$.
      We let $U = \text{null} \ \phi$.
      Since the sum
      of these subspaces is direct, it follows that:

      $$\dim \ V = \dim \ \text{null} \ \phi + \dim \ U' \ \Rightarrow \ \dim U' = \dim \ V - \dim \ \text{null} \ \phi = 1$$

      where we used the equation above. Thus, $U'$ must be a one-dimensional subspace. All one dimensional subspaces
      of some vector space $V$ are all multiples of a single vector, $u$. In addition, since the sum of $U'$ and the null space is direct,
      this vector cannot be in $\text{null} \ \phi$. Therefore:

      $$U' = \{au \ : \ a \in \mathbb{F}\}$$

      and:

      $$V = \text{null} \ \phi \oplus \{au \ : \ a \in \mathbb{F}\}$$

      for some $u \in V$.
      \newline

      Now, the last thing we have to show is that $U'$ can be multiples of \textbf{any} vector not in the null-space (not just $u$).
      Given some $v \in V$, we will have, from above:

      $$v = n + au$$

      for some $n$ in the null space. Given some $w$ also not in the null space, we choose $c$ such that $a \phi(u) - c \phi(w) = 0$, which
      we can do as we know that both $\phi(u)$ and $\phi(w)$ are non-zero. Thus:

      $$n + au = (n + au - cw) + cw = m + cw$$

      where $m$ is in the null space. We prove inclusion the other way in a similar fashion, implying that:

      $$\text{null} \ \phi \ \oplus = \{au \ : \ a \in \mathbb{F}\} = \text{null} \ \phi \ \oplus = \{aw \ : \ a \in \mathbb{F}\}$$

      Therefore, we are able to conclude that:

      $$V = \text{null} \ \phi \oplus \{au \ : \ a \in \mathbb{F}\}$$

      for \textbf{any} $u$ not in the null space.
      

   \end{proof}

    \begin{problem}{3.30}

      Suppose $\phi_1$ and $\phi_2$ are linear maps from $V$ to $\mathbb{F}$ that have the same null space. Show that there exists
      some $c \in \mathbb{F}$ such that $\phi_1 = c\phi_2$.

    \end{problem}

    \begin{proof}

      Using the previous result, we can write $V$ as the sum:

      $$V = \text{null} \ \phi_1 \ \oplus \ \{au : u \in \mathbb{F}\} = \text{null} \ \phi_2 \ \oplus \ \{au : u \in \mathbb{F}\}$$

      Let us pick some $v \in V$. We will have $v = n + au$ where $n$ is in the null-space of both maps. We will have:

      $$\phi_1(v) = \phi_1(n + au) = \phi_1(n) + a\phi_1(u)$$

      We then choose some $c$ such that $\phi_1(u) = c \phi_2(u)$, which we can do as both $\phi_1(u)$ and $\phi_2(u)$ are non-zero. In addition, we will have:
      $\phi_1(n) = \phi_2(n) = 0$, as both maps have the same null-space. We note that $c \phi_2(n) = \phi_2(n)$. Thus, we will have:

      $$\phi_1(n) + a\phi_1(u) = c \phi_2(n) + c \phi_2(au) = c \phi_2(n + au) = c \phi_2(v)$$

      This completes the proof.

    \end{proof}

    \section{Section 3C}

    \begin{problem}{3.6}

      Suppose that $V$ and $W$ are finite-dimensional and $T \in \mathcal{L}(V, \ W)$. Prove that if $\dim \text{range} \ T = 1$ if and only if
      there exists a basis of $V$ and a basis of $W$ such that with respect to these bases, all entires of $\mathcal{M}(T)$ are equal to $1$.

    \end{problem}

    \begin{proof}

      Clearly, if there are bases of $V$ and $W$ such that $\mathcal{M}(T)$ has ones in all entries, then each basis vector in the chosen basis
      will get mapped to the sum of all the chosen basis vectors of $W$, which we call $w$. It follows that $\text{range} \ T = \text{span}(w)$, implying
      that the dimension of the range of $T$ is $1$.
      \newline

      Conversely, assume that $\dim \text{range} \ T = 1$. From rank-nullity theorem, it follows that $\dim \Null T = n - 1$, where
      $n$ is the dimension of $V$. Since the dimension of the range is $1$. There must exist some vector $v$ of $V$ such that
      $T(v) = w$, where $w \neq \boldsymbol 0$. We choose a basis $w_1, \ ..., \ w_m$ of $W$, which means that:

      $$w = a_1 w_1 + \ \cdots \ + a_m w_m$$

      We let the set $\{a_1 w_1, \ ..., \ a_m w_m\}$ be a basis for $W$, and denote the $k$-th element of the basis $w'_k$.
      Now, consider some basis $v_1, \ ..., \ v_{n - 1}$ for the null space of $T$.
      The set of vectors $\{v + v_0, \ v + v_1, \ ..., \ v + v_{n - 1}\}$ (where $v_0 = \boldsymbol 0$) will clearly be a basis for $V$, as each vector in the $n$-element set is linearly independent. We
      denote the $k + 1$-th element of this basis $v'_k$.
      \newline

      Now, consider $T$ acting upon some basis vector:

      $$T(v'_k) = T(v) + T(v_0) = w = w'_1 + \ \cdots \ + w'_m$$

      So in the primed bases, each element of $\mathcal{M}(T)$ is $1$, by definition.

    \end{proof}

    \section{Section 3D}

    \begin{problem}{3.17}
      Suppose $V$ is finite-dimensional and $\mathcal{E}$ is a subspace of $\mathcal{L}(V)$ such that $ST \in \mathcal{E}$ and $TS \in \mathcal{E}$ for
      all $S \in \mathcal{L}(V)$ and all $T \in \mathcal{E}$. Prove that $\mathcal{E} = \{\boldsymbol 0\}$ or $\mathcal{E} = \mathcal{L}(V)$.

    \end{problem}

    \begin{proof}

      Clearly, $\mathcal{E}$ can be the trivial subspace.
      \newline

      Now, consider what happens when we assume that there is some non-zero $T \in \mathcal{E}$. It follows that there must exist some $v \in V$ such that
      $T(v) = w_1$, where $w_1$ is non-zero. Extending $w_1$ to a basis for $V$, we get the set $w_1, \ ..., \ w_n$.
      \newline

      We let $S^k_1$ be the map that takes $w_k$ to $v$ and all other basis elements to $0$. We let $S^k_2$ be the map that takes $w_1$ to $w_k$, and all other
      basis elements to $0$. It follows that the map $T S^k_1$ takes $w_k$ to $w_1$, and all other basis vectors to $0$, and is in $\mathcal{E}$. We can then conclude that
      $S^r_2 T S^k_1$ is also in $\mathcal{E}$, and is the map that takes $w_k$ to $w_r$, and all other basis elements to $0$.
      \newline

      Clearly, any map from $V$ to $V$ can be written as a linear combination of maps of the form $S^r_2 T S^k_1$. Since $\mathcal{E}$ is a subspace, all such
      linear combinations are in $\mathcal{E}$. This implies that $\mathcal{E} = \mathcal{L}(V)$.
      \newline

      It follows that $\mathcal{E}$ is either trivial, or the whole space $\mathcal{L}(V)$.
      \end{proof}

    \section{Section 3E}

    \begin{problem}{3.7}
      If $x, \ v \in V$ and $U, \ W$ are subspaces of $V$, such that $v + U = x + W$, then $U = W$
    \end{problem}

    \begin{proof}
      Clearly, $v \in v + U$. It follows that $v = x + w$, for some $w \in W$. We then have $v - x = w$, so $v - x \in W$.
      \newline

      Now, consider $u \in U$. We will have $v + u = x + w' \ \Rightarrow \ u = x - v + w' = w' - (v - x) = w' - w$, so $u \in W$. Proving
      this each $w \in W$ is in $U$ is identical. Thus, we have inclusion both ways, so $U = W$.
      \end{proof}
    
    \begin{problem}{3.18}

      Suppose that $T \in \mathcal{L}(V, \ W)$ and $U$ is a subspace of $V$. Let $\pi$ denote the quotient map from $V$ onto $V/U$.
      Prove that there exists $S \in \mathcal{L}(V/U, \ W)$ such that $T = S \circ \pi$ if and only if $U \subset \text{null} \ T$.

    \end{problem}

    \begin{proof}

      Assume that there exists $S$ such that $T = S \circ \pi$. Let us pick some $u \in U$. We note that $Tu = (S \circ \pi)(u) = S([u]) = S([0]) = 0$,
      so $U \subset \text{null} \ T$.
      \newline

      Assume that $U \subset \text{null} \ T$. Since $U$ is a subspace of the null space, it follows that for $u \in U$, we have $T(u) = 0$. Thus,
      given $w$ and $v$ in $V$ such that $\pi(w) = \pi(v)$, we can notice that $w - v \in U$, by definition of the quotient space, so

      $$T(w - v) = T(w) - T(v) = 0 \ \Rightarrow T(w) = T(v)$$

      Thus, we define $S$ to be the map that takes $[v]$ in the quotient space to $T(v)$ in $W$. Such a map is well defined as if $[v] = [w]$, then
      $S([w]) = T(w) = T(v) = S([v])$. Clearly, such a map is linear, as:

      $$S([w] + [v]) = S([w + v]) = T(w + v) = T(w) + T(v) = S([w]) + S([v])$$

      and:

      $$\lambda S([w]) = \lambda T(w) = T(\lambda w) = S([\lambda w]) = S(\lambda [w])$$

      and the proof is complete.

    \end{proof}

    \section{Section 3F}

    \begin{prop}
      Let $U^0$ be the annihiltor of $U$ as a subspace of $V$. It follows that:

      $$\dim U^{0} + \dim U = \dim V$$
    \end{prop}

    \begin{proof}

      We attempt to prove this in the language of linear functionals.
      \newline

      We know that $V'$ is the space of functionals from $V$ to $\mathbb{F}$. We know that $U$ is a subspace of $V$,
      so it follows that we can choose a basis $v_1, \ ..., \ v_n$ of $U$, then extend it to a basis for $V$ by adding
      vector $v_{n + 1}, \ ..., \ v_m$.
      \newline

      Using this basis, we can define the dual basis on $V'$ of the elements $\phi_{i}(v_k)$ for $v_k$ in the basis of $V$.
      \newline

      We define a linear map $T : V' \ \rightarrow \ V'$ which takes the basis element $\phi_i$ to itself if $1 \leq i \leq n$ (so the
      corresponding $v_i$ is in $U$), and to $0$ otherwise.
      \newline

      We assert that $\text{null} \ T = U^0$. Let us pick some $\phi \in \text{null} \ T$. We will have:

      $$T(a_1 \phi_1 + \ \cdots \ + a_m \phi_m) = a_1 \phi_1 + \ \cdots \ + a_n \phi_n = 0$$

      Since each element of the dual basis is linearly independent, all $a_k$ must be $0$, thus, $\phi$ is a linear combination of the $\phi_k$ basis elements for $k \geq n + 1$.
      It follows that $\phi(u) = 0$ for all $u \in U$, as $u$ is a linear combination of exclusively the basis elements $v_k$ from $k = 1$ to $k = n$. Thus, $\phi$ is in $U^0$.
      \newline

      Now, if $\phi \in U^0$, it follows that $\phi(u) = 0$ for all $u \in U$, so we will have:

      $$T(\phi) = a_1 T(\phi_1) + \ \cdots \ a_m T(\phi_m) = a_1 \phi_1 + \ \cdots \ + a_n \phi_n$$

      Now, given some $v_k$ for $k$ between $1$ and $n$, we will have:

      $$(a_1 \phi_1 + \ \cdots \ + a_n \phi_n)(v_k) = a_k \phi_k(v_k) = a_k = 0$$

      so each $a_k$ is equal to $0$, implying that $T(\phi)$ is the zero map, so $\phi$ is in the null space. Thus, $U^0 = \text{null} \ T$.
      \newline

      Finally, using the fundmanetal theorem of linear maps:

      $$\dim V' = \dim \ \text{range}(T) + \dim \ \text{null}(T) = \dim \ U' + \dim U^0$$

      But we know that $\dim V' = \dim V$ and $\dim U' = \dim U$, so:

      $$\dim V = \dim U + \dim U^0$$

    \end{proof}

    \begin{problem}{}

      \end{problem}

    \section{Section 5A}

    \begin{prop}
      Given a set of $m$ distinct eigenvalues $\lambda_1, \ ..., \ \lambda_m$, along with a set of corresponding eigenvectors $V = \{v_1, \ ..., \ v_m\}$, the
      set $V$ is linearly independent.
    \end{prop}

    \begin{proof}

      We will prove this proposition by induction. Clearly, this will be true in the case of one eigenvalue, $\lambda$. Assume that
      it holds true given $n$ eigenvalues. We prove it holds true for $n + 1$.

      Consider the set of eigenvalues $\{\lambda_1, \ ..., \ \lambda_{n + 1}\}$ with
      corresponding eigenvectors $\{v_1, \ ..., \ v_{n + 1}\}$.
      Assume that there is a non-trivial linear combination:

      $$a_{1} v_1 + \ \cdots \ + a_{n} v_{n} + a_{n + 1} v_{n + 1} = 0$$

      Note that since eigenvectors are non-zero, for this non-trivial linear combination to be $0$, we must have at least two $a_i$ not equal to
      $0$ otherwise we would have $a_k v_k = 0$, for non-zero $a_k$, which can't be the case. It follows that at least one $a_i$ with $1 \leq i \leq n$
      is non-zero.


      We define the linear operator $(T - \lambda_{n + 1} I)$. We then have:

      $$(T - \lambda_i I)(a_1 v_1 + \ \cdots \ + a_n v_n + a_{n + 1} v_{n + 1}) = \displaystyle\sum_{k \neq n + 1} a_k (\lambda_k - \lambda_{n + 1}) v_k = 0$$

      But since all eigenvalues are unique, we must have $\lambda_k - \lambda_{n + 1} \neq 0$. In addition, it least one $a_i$ in this sum is non-zero. Thus,
      we have found a non-trivial linear combination of $n$ eigenvectors that yields the zero vector, a contradiction to the inductive hypothesis.
      \newline

      It follows that the set $\{v_1, \ ..., \ v_{n + 1}\}$ is linearly independent and the proof is complete.

      \end{proof}

    \begin{problem}{5.28}
      Suppose $V$ is finite-dimensional with $\dim V \geq 3$ and $T \in \mathcal{L}(V)$ is such that every 2-dimensional subspace of $V$ is
      invariant under $T$. Prove that $T$ is a scalar multiple of the identity operator.
    \end{problem}

    \begin{proof}

      Consider some $v \in V$. Since the dimension of $V$ is greater than or equal to $3$, we can also choose
      two other vectors, $w$ and $z$ that form a linearly independent set $\{v, \ w, \ z\}$.
      We consider the two-dimensional subspaces $A = \text{span}(v, \ w)$ and $B = \text{span}(v, \ z)$. We know that
      $A$ is invariant, so it follows that $Tv = av + bw$, but we know that $B$ is also invariant, so $Tv = cv + dz$. This implies that:

      $$(c - a)v + dz - bw = 0$$

      and since these vectors are linearly independent, we have $d = b = 0$, so it follows that $v$ is sent to a multiple of itself.
      \newline

      Now, we pick linearly independent $v$ and $w$ in $V$ such that $T v = a v$ and $T w = b w$. We will have:

      $$T(v + w) = c(v + w) = T(v) + T(w) = av + bw$$

      so since $v$ and $w$ are linearly independent, it follows that $c = a = b$, so $Tv = cv$ and $Tw = cw$. Thus, $T$ is a scalar multiple of the
      identity map and the proof is complete.

    \end{proof}

    \begin{problem}{5.35}
      Suppose $V$ is finite-dimnensional, $T \in \mathcal{L}(V)$, and $U$ is invariant under $T$. Prove that each eigenvalue of $T/U$ is an eigenvalue of $T$.
    \end{problem}

    \begin{proof}
      Clearly, if $\lambda$ is an eigenvalue of $T$, then there is $v$ such that $Tv = \lambda v$, so it follows that $Tv - \lambda v = 0$, implying that
      $Tv - \lambda v \in U$, so:

      $$Tv + U = \lambda v + U \ \Rightarrow \ (T/U)(v + U) = \lambda (v + U)$$

      so $\lambda$ is an eigenvalue of $T/U$.
      \newline

      Conversely, let us assume that is $v + U$ such that $(T/U)(v + U) = T(v) + U = \lambda v + U$. This
      implies that $Tv - \lambda v \in U$.
      \newline

      To do this, it is enough to show that the map $T - \lambda I$ is not surjective. 

    \end{proof}

    \section{Section 8}

    \begin{prop}
      Given some operator $T : V \ \rightarrow \ V$ with $V$ finite dimensional, there is a minimal polynomial $p(z)$ such that
      $p(T) = 0$. In other words, there exists a polynomial such that for all $q(z)$, with $q(T) = 0$, we have:

      $$q(z) = t(z) p(z)$$

      for some polynomial $t(z)$
    \end{prop}

    \begin{proof}

    \end{proof}

    \begin{prop}
      
      \end{prop}

    \end{document}
