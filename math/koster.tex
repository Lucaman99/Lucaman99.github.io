\documentclass[10pt, oneside]{article} 
\usepackage{amsmath, amsthm, amssymb, wasysym, verbatim, bbm, color, graphics, geometry, hyperref, biblatex, mathtools, xcolor}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{tcolorbox}

\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	urlcolor=blue
}

\addbibresource{ref.bib}

\geometry{tmargin=.75in, bmargin=.75in, lmargin=1in, rmargin =1in}
\setlength\parindent{0pt}

\tcbuselibrary{theorems}
\newtcbtheorem
    []% init options
    {problem}% name
    {Problem}% title
    {%
      fonttitle=\bfseries,
    }% options
    {prob}% prefix

    \newcommand{\R}{\mathbb{R}}
    \newcommand{\C}{\mathbb{C}}
    \newcommand{\Z}{\mathbb{Z}}
    \newcommand{\N}{\mathbb{N}}
    \newcommand{\Q}{\mathbb{Q}}
    \newcommand{\Cdot}{\boldsymbol{\cdot}}

    \newtheorem{thm}{Theorem}
    \newtheorem{defn}{Definition}
    \newtheorem{conv}{Convention}
    \newtheorem{rem}{Remark}
    \newtheorem{lem}{Lemma}
    \newtheorem{cor}{Corollary}
    \newtheorem{prop}{Proposition}

    \newcommand{\tr}{\mathrm{Tr}}


    \title{Challenge Accepted, Matt}
    \author{Jack Ceroni}
    \date{December 2020}

    \begin{document}

    \maketitle
    \tableofcontents

    \vspace{.25in}

    \newpage

    \section{Problem 1}

    \begin{prop}
      The degree of $(x - a_1) \cdots (x - a_n)$ is $n$, for $a_k \in \mathbb{F}$.
    \end{prop}

    \begin{proof}
      Clearly, this product does not have a non-zero term of the form $a x^{k}$, for $k > n$ (this can be formally demonstrated using induction).
      In addition, this
      polynomial has a term of the form $x^n$.
      \newline

      There exists no field in which $1 = 0$, so it follows that
      $x^n$ is a non-zero term in the expansion.
    \end{proof}

    \textbf{Part 1}
    \newline

    Assume that $p(x)$ does split over $\mathbb{R}$. We then must have $x^2 + 1 = (x - a)(x - b)$, as if there were any more terms
    in the product, the degree of the resulting polynomial would be greater than $2$.
    \newline

    We then have:

    $$x^2 + 1 = x^2 - (a + b)x + ab$$

    so $a = -b$ and $ab = -a^2 = 1$, which implies that $a^2 = -1$. However, from the axioms of $\mathbb{R}$, the square of any $r \in \mathbb{R}$ must be
    positive, so this is a contradiction. It follows that $p(x)$ cannot be split over $\mathbb{R}$.
    \newline

    \textbf{Part 2}
    \newline

    Assume $q(x) = x^2 + x + 1$ does split over $\mathbb{F}_2$. By Proposition 1:

    $$x^2 + x + 1 = (x - a)(x - b) = x^2 - (a + b)x + ab$$

    So $-a - b = 1$ and $ab = 1$. The second equation implies that we must have $a = b = 1$, but $-1 + (-1) = 0 \neq 1$, so we have a contradiction.
    Thus, $q(x)$ does not split over $\mathbb{F}_2$.
    \newline

    \textbf{Part 3}
    \newline

    Let $T$ be a map such that $p_T(x)$ splits. It follows that $T$ can be put in Jordan form. In other words, there exists a basis such that the
    matrix of $T$ with respect to this basis is in Jordan form.
    \newline

    \textcolor{red}{There is no notion of similarity between linear maps, only matrices, so this question is not well-defined.}
    \newline

    \textbf{Part 4}
    \newline

    Pick some matrix $\mathcal{A} \in M_n(\mathbb{C})$. We define a linear operator $A : \mathbb{F}^{n \times 1} \ \rightarrow \ \mathbb{F}^{n \times 1}$, over the field $\mathbb{C}$, such
    that $A(v) = \mathcal{A}v$ (clearly such a map is linear, by definition of matrix multiplication). 
    \newline

    It is easy to see that if $\beta$ is the standard
    basis on $\mathbb{F}^{n \times 1}$, then $\mathcal{A} = {}_{\beta} [A]_{\beta}$
    \newline

    We let $p_A$ be the minimal polynomial of $A$. We know that any polynomial can be factored over the complex field, so it follows that $p_A$ splits. Therefore, from
    Part 3, there exists a basis $\beta'$ such that the matrix of $A$ with respect to this basis is in Jordan form. In other words:

    $${}_{\beta'} [A]_{\beta'} = {}_{\beta'} I_{\beta} {}_{\beta} [A]_{\beta} {}_{\beta} I_{\beta'} = P^{-1} \mathcal{A} P$$

    is in Jordan form, where $P = {}_{\beta} I_{\beta'}$. Thus, by definition, $\mathcal{A}$ is similar to a matrix in Jordan form and the proof is complete.
    \newline

    \textbf{Part 5}

    \begin{prop}
      If $A$ is an upper-diagonal matrix, then $A^n$ is also upper-diagonal, for any natrual $n$.
    \end{prop}

    \begin{proof}
      Clearly, this is true in the case of $n = 1$. Assume the case of $n$. For the case of $n + 1$, we note that:

      $$A_{ij}^{n + 1} = \displaystyle\sum_{r} A_{ir}^{n} A_{rj}$$

      Assume that $i > j$ (these are entries below the upper diagonal). If $r \geq i$, then $r > j$, so $A_{rj} = 0$ and $A_{ij}^{n + 1} = 0$. If $r < i$, then
      from the inductive hypothesis, $A_{ir}^{n} = 0$, so $A_{ij}^{n + 1} = 0$. Thus, $A^{n + 1}$ is upper-diagonal and the proof by induction is complete.
    \end{proof}

    \begin{prop}
      If $A$ is upper-diagonal with diagonal entries $A_{kk}$, then for the diagonal entires of $A^{n}$ (which we call $A_{kk}^n$), we have $A_{kk}^{n} = (A_{kk})^n$.
      \end{prop}

    \begin{proof}
    Clearly, this is true in the case of $k = 1$. Assume the case of $k = n$. Consider the case of $k = n + 1$. The entries of the matrix $A^{n + 1}$ will be given by:

    $$A_{ij}^{n + 1} = \displaystyle\sum_{r} A_{ir}^{n} A_{rj}$$

    So for $i = j$, we will have:

    $$A_{ii}^{n + 1} = \displaystyle\sum_{r} A_{ir}^{n} A_{ri}$$

    Clearly $A^n$ is upper-diagonal, so for $r < i$, we will have $A_{ir}^{n} = 0$. In the case of $r > i$, we have $A_{ri} = 0$. Thus, the only
    term in the sum that can be non-zero is $r = i$, By the inductive hypothesis:

    $$\displaystyle\sum_{r} A_{ir}^{n} A_{ri} = A_{ii}^{n} A_{ii} = (A_{ii})^n A_{ii} = (A_{ii})^{n + 1}$$

    and the proof is complete.
    \end{proof}

    \section{Problem 2}

    \textbf{Part 1}
    \newline

    We will have, from the defintition

    $$e^U = \displaystyle\sum_{n = 0}^{\infty} \frac{U^n}{n!}$$

    where we define $U^0 = I$. The matrix sum in entry-wise, so if we let $E_{rj}$ be the $(r, \ j)$-th element
    of $e^U$ and $U^n_{rj}$ be the $(r, \ j)$-th entry of $U^n$, then we will have:

    $$E_{rj} = \displaystyle\sum_{n = 0}^{\infty} \frac{U^{n}_{rj}}{n!}$$

    We note that from Problem 5, we will have $U^{n}_{rr} = a_{r}^n$, so it follows that:

    $$E_{rr} = \displaystyle\sum_{n = 0}^{\infty} \frac{a_{r}^n}{n!} = e^{a_r}$$

    by definition of the function $e^x$. This completes the proof.
    \newline

    \textbf{Part 2}
    \newline

    \begin{prop}
      For some natrual $n$ and some arbitrary $A$:

      $$(Q A Q^{-1})^n = Q A^n Q^{-1}$$
    \end{prop}

    \begin{proof}
      Clearly, this is true for $n = 1$. Assume the case of $n$. For $n + 1$, we have:

      $$(Q A Q^{-1})^{n + 1} = (Q A Q^{-1}) (Q A Q^{-1})^{n} = (Q A Q^{-1})(Q A^n Q^{-1}) = Q A^{n + 1} A^{-1}$$

      and the proof by induction is complete.
      \end{proof}

    We will have:

    $$\exp(Q A A^{-1}) = \displaystyle\sum_{n = 0}^{\infty} \frac{(Q A Q^{-1})^n}{n!} = \displaystyle\sum_{n = 0}^{\infty} \frac{Q A^n Q^{-1}}{n!} =
    Q \Big[ \displaystyle\sum_{n = 0}^{\infty} \frac{A^n}{n!} \Big] Q^{-1} = Q e^A Q^{-1}$$

    \textbf{Part 3}
    \newline

    \begin{prop}
      $e^{x + y} = e^x e^y$
    \end{prop}

    \begin{proof}
      $$e^{x + y} = \displaystyle\sum_{n = 0}^{\infty} \frac{(x + y)^n}{n!} = \displaystyle\sum_{n = 0}^{\infty} \frac{1}{n!} \displaystyle\sum_{j = 0}^{n} {n \choose j} x^{j} y^{n - j} =
      \displaystyle\sum_{n = 0}^{\infty} \displaystyle\sum_{j = 0}^{n} \frac{x^j}{j!} \frac{y^{n - j}}{(n - j)!}$$

      We make the claim that:

      $$\displaystyle\sum_{n = 0}^{\infty} \displaystyle\sum_{j = 0}^{n} \frac{x^j}{j!} \frac{y^{n - j}}{(n - j)!} = \displaystyle\sum_{p = 0}^{\infty} \displaystyle\sum_{q = 0}^{\infty} \frac{x^p}{p!} \frac{y^q}{q!}$$

      Clearly, given some pair $(p, \ q)$ characterizing a unique term of the right-hand sum, there will exist a unique term in the left-hand sum with $n = p + q$ and $j = p$ that is equal to this term.
      \newline

      In addition, given some pair $(n, \ j)$ characterizing a unique term in the left-hand sum, there exists a unique term in the right-hand sum with $p = j$ and $q = n - j$ that is equal to this term.
      \newline

      Thus, there is a one-to-one correspondence between the terms of the sums, so:

      $$\displaystyle\sum_{n = 0}^{\infty} \displaystyle\sum_{j = 0}^{n} \frac{x^j}{j!} \frac{y^{n - j}}{(n - j)!} = \displaystyle\sum_{p = 0}^{\infty} \displaystyle\sum_{q = 0}^{\infty} \frac{x^p}{p!} \frac{y^q}{q!} =
      \Big( \displaystyle\sum_{p = 0}^{\infty} \frac{x^p}{p!} \Big) \Big( \displaystyle\sum_{q = 0}^{\infty} \frac{x^q}{q!} \Big) = e^{x} e^{y}$$

      and the proof is complete.
      \newline

      Note that this proof can easily be generalized to aribtrary sums, using induction.
      \end{proof}

    From Problem 4, we note that $A$ is similar to an upper triangular matrix, so $A = Q B Q^{-1}$, where $B$ is upper-triangular. It follows that:

    $$\det(e^A) = \det( e^{Q B Q^{-1}} ) = \det( Q e^B Q^{-1} )$$

    Since the determinant is invariant under change of basis, it follows that:

    $$\det(Q e^{B} Q^{-1}) = \det(e^{B})$$

    Since trace is invariant under change of basis, it follows that $\text{trace}(A) = \text{trace}(B) = 0$, which implies that:

    $$\displaystyle\sum_{k} b_{kk} = 0$$

    where $b_{ij}$ is the $(i, \ j)$-th element of the matrix $B$.
    \newline

    The determinant of an upper-triangular matrix is simply the product of the diagonal entires. Since $e^B$ is a sum of powers of an
    upper-diagonal matrix, it is also upper-diagonal, so its determinant will be the product of its diagonal.
    \newline

    Therefore, from Part 1 and Proposition 4:

    $$\det(e^{B}) = \displaystyle\prod_{k} e^{b_{kk}} = \exp \Big[ \displaystyle\sum_{k} b_{kk} \Big] = e^0 = 1$$

    and the proof is complete, $\det(e^A) = \det(e^B) = 1$. Note that it is easy to see that $e^0 = 1$, from the definition of the exponential function.

    \section{Problem 3}

    \textbf{Part 1}
    \newline

    Clearly, the commutator is a valid bilinear map:

    $$[A + B, \ Y] = (A + B)Y - Y(A + B) = AY - YA + BY - YB = [A, \ Y] + [B, \ Y]$$
    $$[\lambda X, \ Y] = \lambda X Y - Y (\lambda X) = \lambda [X, \ Y]$$

    where we can verify that the same linearity holds true for the second entry in a similar fashion.
    \newline
    
    In addition, given $X \in GL(n, \ \mathbb{R})$, we have $[X, \ X] = XX - XX = 0$. Finally:

    $$[X, \ [Y, \ Z]] + [Z, \ [X, \ Y]] + [Y, \ [Z, \ X]] = [X, \ YZ - ZY] + [Z, \ XY - YX] + [Y, \ ZX - XZ]$$
    $$ = XYZ - XZY - YZX + ZYX + ZXY - ZYX - XYZ + YXZ + YZX - YXZ - ZXY + XZY = 0$$

    Trust me, all the terms cancel. Therefore, $GL(n, \ \mathbb{R})$ equipped with the commutator
    is a real Lie algebra (we already know that $M_{n}(\mathbb{R})$ is a vector space over $\mathbb{R}$).
    \newline

    The dimension of $GL(n, \ \mathbb{R})$ is $n^2$, as it is easy to verify that the list of matrices
    with a $1$ in entry $(i, \ j)$ and $0$s everywhere else, for all $i, \ j$ from $1$ to $n$ is a linearly independent spanning set, and
    has $n^2$ elements.
    \newline

    \textbf{Part 2}
    \newline

    \begin{prop}
      $\mathrm{trace}(XY) = \mathrm{trace}(YX)$
    \end{prop}

    \begin{proof}
      $$\text{trace}(XY) = \displaystyle\sum_{k} (XY)_{kk} = \displaystyle\sum_{k} \displaystyle\sum_{r} X_{kr} Y_{rk} = \displaystyle\sum_{r} \displaystyle\sum_{k} Y_{rk} X_{kr} = \displaystyle\sum_{r} (YX)_{rr} = \text{trace}(YX)$$
      \end{proof}

    First, we note that given $X$ and $Y$ in the vector space, we will have:

    $$\text{trace}(XY - YX) = \text{trace}(XY) - \text{trace}(YX) = \text{trace}(XY) - \text{trace}(XY) = 0$$

    Thus, $[X, \ Y]$ is an element of $SL(n, \ \mathbb{R})$, so the commutator is a valid bilinear map.
    \newline

    Now, it is sufficient to show that $SL(n, \ \mathbb{R})$ is a vector space, as we have already proved the sufficient propoerties of the commutator
    above.
    \newline

    Clearly, given two trace $0$ matrices, their vector sum (component-wise addition) will result in a matrix that also has trace $0$. The
    same is clearly true for component-wise scalar multiplication. Finally, it is clear that the $0$ matrix is in $SL(n, \ \mathbb{R})$. Thus, it is a vector space.
    \newline

    It is easy to verify that trace is a linear map. Clearly, $SL(n, \ \mathbb{R})$ is the null-space of the trace operator when it maps from
    $M_n(\mathbb{R})$ to $\mathbb{F}$. Thus, by rank-nullity theorem:

    $$\dim \ M_n(\mathbb{R}) = \dim \ SL(n, \ \mathbb{R}) + \dim \ \mathbb{F} \ \Rightarrow \ \dim \ SL(n, \ \mathbb{R}) = n^2 - 1$$

    where we note that $\text{range} \ \text{trace} = \mathbb{F}$, as there exists a matrix with non-zero trace $\lambda$, and all other elements of $\mathbb{F}$ will
    simply be scalar multiples of $\lambda$.
    \newline

    \textbf{Part 3}
    \newline

    Clearly, $[X, \ Y] = 0$ is a valid bilinear map from $V$ to $V$, as $0 \in V$. In addition, $[X, \ X] = 0$, by definition of the map.
    \newline

    It is easy to verify that the Jacobi identity also holds.
    \newline

    \textbf{Part 4}
    \newline

    Consider $X, \ Y \in SO(n, \ \mathbb{R})$. We note, from the basic properties of transposition:

    $$(XY - YX)^{T} = (XY)^{T} - (YX)^{T} = Y^{T} X^{T} - X^{T} Y^{T} = (-Y)(-X) - (-X)(-Y) = YX - XY = -(XY - YX)$$

    Thus, the given bracket is a valid bilenear map. It is easy to verify that set $SO(n, \ \mathbb{R})$ is closed under
    scalar multiplication and vector addition, and contains the $0$ matrix. Therefore, $SO(n, \ \mathbb{R})$ is a vector space.
    \newline

    \begin{prop}
      The dimension of $SO(n, \ \mathbb{R})$ is $n(n - 1)/2$.
    \end{prop}

    \begin{proof}
      Consider the set $B$ of matrices $m$:

      $$B = \{m \ | \ m \in M_{n}(\mathbb{R}), \ m_{ij} = -1, \ m_{ji} = 1\}$$

      with $0$s in all other entries, and $1 \leq i \leq n$ and $j < i$. Clearly, every element of $B$ is in $SO(n, \ \mathbb{R})$, so $\text{span}(B) \subset SO(n, \ \mathbb{R})$.
      In addition, consider some $M \in SO(n, \ \mathbb{R})$. We must have:

      $$M^{T} = -M \ \Rightarrow \ M_{ij} = -M_{ji}$$

      for all $i$ and $j$ from $1$ to $n$. We note that the above
      equation implies that $M_{ii} = -M_{ii}$, so $M_{ii} = 0$. In other words, the main diagonal of $M$ is all $0$s.

      Let $\mathcal{M}^{ij}$ be the matrix with a $1$ in entry $(i, \ j)$ and $0$s everywhere else. Using the above facts, we will have:

      $$M = \displaystyle\sum_{i, \ j} M_{ij} \mathcal{M}^{ij} = \displaystyle\sum_{i = 1}^{n} \displaystyle\sum_{j < i} M_{ij} (\mathcal{M}^{ij} - \mathcal{M}^{ji})$$

      By definition, $\mathcal{M}^{ij} - \mathcal{M}^{ji}$ is an element of $B$, so it follows that $B$ is a linear combination of elements of $B$. Therefore, $SO(n, \ \mathbb{R}) \subset \text{span}(B)$.
      \newline

      We have inclusion both ways, so $SO(n, \ \mathbb{R}) = \text{span}(B)$. Finally, we note that all elements of $B$ are linearly independent, as each matrix in $B$
      contains a non-zero entry in some entry $(i, \ j)$ where no other element of $B$ has a non-zero entry.
      \newline

      Thus, by definition, $B$ is a basis for $SO(n, \ \mathbb{R})$. Clearly, there are ${n \choose 2} = n(n - 1)/2$ elements in $B$, so the dimension of $SO(n, \ \mathbb{R})$ is $n(n - 1)/2$. This
      completes the proof.
    \end{proof}

    \textbf{Part 5}
    \newline

    Using a very similar proof to Proposition $3$, it is clear that the product of two upper diagonal matrices with $0$s on the main diagonal is also an upper-diagonal
    matrix with $0$s on the main diagonal. Thus, given $X$ and $Y$ in the Heisenberg group, $XY - YX$ is also in the Heisenberg group as well.
    \newline

    It follows that $[X, \ Y]$ is a valid bilinear map into the Heisenberg group.
    \newline

    It isn't difficult to see that $H(3, \mathbb{R})$ is closed under vector addition and scalar multiplication, and contains the $0$ vector. Therefore, $H(3, \ \mathbb{R})$ is
    in fact a real Lie algebra.
    \newline

    Clearly:

    $$B = \Big( \begin{pmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}, \ \begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}, \ \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{pmatrix} \Big)$$

    is a basis for $H(3, \ \mathbb{R})$ so the Lie algebra has dimension $3$.
    \newline

    \textbf{Part 6}
    \newline

    Using the component-wise definition of the cross product in $\mathbb{R}^3$, one can verify that it does in fact alternate and satisfies the Jacobi identity (I omit these computations for brevity).
    \newline

    Clearly, the
    result will also be an element of $\mathbb{R}^3$. Thus, $\mathbb{R}^3$ with $[] = \times$ is a valid Lie algebra.
    \newline

    \textbf{Part 7}
    \newline

    We first write down the bases for both Lie algebras:

    $$B_{\times} = ((1, \ 0, \ 0), \ (0, \ 1, \ 0), \ (0, \ 0, \ 1))$$

    $$B_{SO} = \Big( \begin{pmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ -1 & 0 & 0 \end{pmatrix}, \ \begin{pmatrix} 0 & 1 & 0 \\ -1 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}, \ \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & -1 & 0 \end{pmatrix} \Big)$$

    Let $v_1, \ v_2, \ v_3$ be the elements of the first basis, and $e_1, \ e_2, \ e_3$ be the elements of the second basis. We define a map from $\mathbb{R}^3$ to $SO(n, \ 3)$ by extending the bijection
    that sends $v_k$ to $e_k$ to a linear map.
    \newline

    We then note from some computation that $[v_1, \ v_2] = v_3$, \ $[v_2, \ v_3] = v_1$, and $[v_3, \ v_1] = v_2$.
    \newline

    In addition, we find that $[e_1, \ e_2] = e_3$, \ $[e_2, \ e_3] = e_1$, and $[e_3, \ e_1] = e_2$. Therefore, we note that:

    $$T([e_{i}, \ e_{j}]) = T(e_k) = v_k = [v_{i}, \ v_{j}] = [T(e_i), \ T(e_j)]$$

    so it follows that $T$ is a valid isomorphism between the Lie algebras.
    \newline

    \textbf{Part 8}
    \newline

    We already know that $\wedge(v)$ is a vector space. We then note that given two elements of $\wedge(v)$ of the form $[v_1]$ and $[v_2]$, we will have:

    $$[v_1] \wedge [v_2] - [v_2] \wedge [v_1] = \pi(v_1 \otimes v_2) - \pi(v_2 \otimes v_1) = \pi(v_1 \otimes v_2 - v_2 \otimes v_1)$$

    $$ = \pi(v_1 \otimes v_2 - v_2 \otimes v_1) + \pi(v_1 \otimes v_1) - \pi(v_2 \otimes v_2) = \pi((v_1 - v_2) \otimes (v_1 + v_2)) = [0]$$

    Therefore, this is a valid Abelian Lie algebra (this automatically confirms the Jacobi identity).
    \newline

    \textbf{Part 9}
    \newline

    \textit{I screwed up the notation a bit by not using the fancy symbols to denote $GL$, $SL$ and $SO$, so I'll refer to $SL$ (as it is used in the notes) as $S$.}
    \newline

    In Problem 2, we showed that if $A$ has trace equal to $0$, then $\det(e^A) = 1$. It follows that given some element of $SL(n, \ \mathbb{C})$ (which has trace equal to $0$), then
    the determinant of the exponential of this element is $1$, so it is in $S(n, \ \mathbb{C})$. Thus, $\exp : SL(2, \ \mathbb{C}) \ \rightarrow \ S(n, \ \mathbb{C})$ is a valid map.
    \newline

    Now, let us pick some element $M \in S(n, \ \mathbb{C})$. Since this vector space is over the complex field, it follows that $M$ is similar to a matrix in Jordan form. Thus,
    $M = Q^{-1} P Q$, where $P$ is in Jordan form.
    \newline

    It follows that $P$ either has $2$ eigenvalues, and is diagonal (with determinant $1$, as determinant is invariant under change of basis), or has $1$ eigenvalue ($1$ or $-1$) and is of the form:

    \begin{equation}
      P = \begin{pmatrix} \pm 1 & 1 \\ 0 & \pm 1 \end{pmatrix}
      \end{equation}

    For the purpose of this exercise, we will assume that the function $f : \mathbb{C} \ \rightarrow \ \mathbb{C}$ with $f(x) = e^x$ is surjective (this follows from
    Euler's formula).
    \newline

    If $P = \text{diag}(a, \ b)$, we note that $\det(P) = ab = 1$. We choose the matrix $N$ to have an entry $x$ in the top right corner such that $e^x = a$ (this follows from the
    surjectivity of $e^x$). We then choose $N$ to have the entry $-x$ in the bottom right corner. We note that $e^{-x} = b$, as $b$ is the unique multiplicative inverse of $a$, and:

    $$a e^{-x} = e^{x} e^{-x} = e^{0} = 1$$

    Clearly, $N$ has trace $0$, so it is in $SL(n, \ \mathbb{C})$. We know that trace is invariant under change of basis, so $Q^{-1} N Q$ is also in $SL(n, \ \mathbb{R})$. Finally, we
    note that:

    $$\exp(Q^{-1} N Q) = Q^{-1} \exp(N) Q = Q^{-1} \exp \Big[ \begin{pmatrix} x & 0 \\ 0 & -x \end{pmatrix} \Big] Q = Q^{-1} \exp \Big[ \begin{pmatrix} e^x & 0 \\ 0 & e^{-x} \end{pmatrix} \Big] Q$$

    $$ = Q^{-1} \exp \Big[ \begin{pmatrix} a & 0 \\ 0 & b \end{pmatrix} \Big] Q = Q^{-1} P Q = M$$

    In addition, it is easy to verify that:

    $$\exp \Big[ \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} \Big] = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix} $$

    Finally, consider the matrix:

    $$T = \begin{pmatrix} -i \pi & -1 \\ 0 & i \pi \end{pmatrix}$$

    it isn't difficult to see that for each term of $e^T$ with an even power of $T$, the top right entry of the matrix is $0$, and for any odd power,
    $k$, the entry is the $k - 1$-th entry of $-e^{i\pi}$
    (this can be proved formally, but it is straightforward).
    \newline

    But by Euler's formula, the sum of all such entries is the real part of $-e^{i\pi}$, which is equal to $-\cos(\pi) = 1$.
    \newline

    Therefore:

    $$\exp \Big[ \begin{pmatrix} i\pi & -1 \\ 0 & -i\pi \end{pmatrix} \Big] = \begin{pmatrix} -1 & 1 \\ 0 & -1 \end{pmatrix}$$

    Therefore, if $M = Q^{-1} P Q$, with $P$ of the form of equation (1), then there exists an element of the domain $T$ such that $e^{T} = M$.
    \newline

    It follows that $\exp$ is surjective, and the proof is complete.
    \newline

    \section{Problem 4}

    \textbf{Part 1}
    \newline

    Clearly, $[g, \ g]$ (all linear combinations of brackets) is closed under vector addition and scalar multiplication, and contains the $0$ vector, so it is a vector subspace.
    \newline

    Since $[g, \ g] \subset g$, it follows from the definition of the bracket that $[[g, \ g], \ [g, \ g]] \subset [g, \ g]$. Therefore, $[g, \ g]$ is a Lie subalgebra. Finally,
    note once again hat since $[g, \ g] \subset g$, we have $[g, \ [g, \ g]] \subset [g, \ g]$. Therefore, $[g, \ g]$ is an ideal.

    \begin{prop}
      Given a Lie algebra $g$, it follows that $[g, \ g]$ is a Lie subalgebra.
    \end{prop}

    \begin{proof}
      Clearly, $[g, \ g] \subset g$, and $[[g, \ g], \ [g, \ g]] \subset g$, so $[g, \ g]$. In addition, we note that given $[A_1, \ B_1]$ and
      $[A_2, \ B_2]$ in $[g, \ g]$, we have:

      \end{proof}

    \textbf{Part 2}
    \newline

    Clearly, $\{0\}$ is a Lie subalgebra of $g$, as it is a vector subspace and $[0, \ 0] = 0$. We then note that given $0 \in \{0\}$ and $G \in g$, we will have:

    $$[G, \ 0] = [G, \ G - G] = [G, \ G] - [G, \ G] = 0$$

    so $[g, \ \{0\}] \subset \{0\}$. Therefore, $\{0\}$ is an ideal.
    \newline

    \textbf{Part 3}
    \newline

    Recall that an Abelian Lie algebra is, by definition, an $n$-dimensional vector space $V$ equipped with the bracket $[X, \ Y] = 0$.
    \newline

    Consider some arbitrary vector subspace $U \subset V$. Clearly, given $u, \ v \in U$, we will have $[u, \ v] = 0$, which is in $U$, so
    $U$ is a Lie subalgebra. Furthermore, given $u \in U$ and $v \in V$, we have $[v, \ u] = 0 \in V$, so $U$ is an ideal.
    \newline

    Therefore, all vector subspaces of $V$ are ideal under the Abelian Lie algebra.
    \newline

    \textbf{Part 4}
    \newline

    \begin{prop}
      The only one-dimensional Lie algebra is the Abelian one.
    \end{prop}

    \begin{proof}
      Let $V$ be a one-dimensional vector space, so $V = \text{span}(v)$ over some field $\mathbb{F}$, where $v$ is some element of $V$.
      \newline

      Let $[\cdot, \ \cdot]$ be a bilinear map from $V \times V$ to $V$. Assume that this bracket along with $V$ form a Lie algebra. Clearly,
      given two elements $av$ and $bv$ of $V$, we must have:

      $$[av, \ bv] = a[v, \ bv] = ab[v, \ v] = ab \cdot 0 = 0$$

      Therefore, the bracket must take all pairs of elements in $V$ to $0$. It follows that if $V$, along with some bracket form a Lie algebra, then
      it must be the Abelian Lie algebra.
    \end{proof}

    \textbf{Part 5}
    \newline

    Consider some Lie algebra $g$. Clearly, an arbitrary element of $[g, \ g]$ will be of the form:

    $$[A, \ B] = \Big[ \displaystyle\sum_{i} a_i e_i, \ \displaystyle\sum_{j} b_{j} e_{j} \Big] = \displaystyle\sum_{i, \ j} a_{i} b_{j} [e_i, \ e_j] = \displaystyle\sum_{i} \displaystyle\sum_{i < j} (a_{i} b_{j} - a_{j} b_{i}) [e_i, \ e_j]$$

    Therefore, all linear combinations of all such $[A, \ B]$ will be a linear combinations of elements of the form $[e_i, \ e_j]$.
    In addition, we know that $[g, \ g]$ is a vector subspace, so any linear combination of element of the form $[e_i, \ e_j]$ will also be in $[g, \ g]$.
    \newline

    Therefore, the set of all $[e_i, \ e_j]$ with $i < j$ forms
    a basis for $[g, \ g]$, so to find $[g, \ g]$ for each of the outlined vector spaces, we simply have to choose a basis $e_1, \ ..., \ e_n$ for $g$, and compute all brackets, then conclude that
    $[g, \ g]$ will simply be the span of all such brackets.
    \newline

    \textbf{Part 5a}
    \newline

    We choose the following basis:

    $$B = \Big( \begin{pmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}, \ \begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}, \ \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{pmatrix} \Big)$$

    and note that $[e_1, \ e_2] = 0$, $[e_1, \ e_3] = 0$, and $[e_2, \ e_3] = e_1$. Therefore:

    $$[H(3, \ \mathbb{R}), \ H(3, \ \mathbb{R})] = \text{span}(e_1)$$

    \textbf{Part 5b}
    \newline

    It is easy to verify that the following set is a basis for $SL(2, \ \mathbb{R})$:

    $$B = \Bigg( \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}, \ \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}, \ \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix} \Bigg)$$

    We then calculate $[e_1, \ e_2] = -2e_2$, $[e_1, \ e_3] = 2e_3$, and $[e_2, \ e_3] = -e_1$. Therefore, $[SL(2, \ \mathbb{R}), \ SL(2, \ \mathbb{R})] = SL(2, \ \mathbb{R})$.
    \newline

    \textbf{Part 5c}
    \newline

    An identical process to above allows us to conclude that $[SL(2, \ \mathbb{C}), \ SL(2, \ \mathbb{C})] = SL(2, \ \mathbb{C})$.
    \newline

    \textbf{Part 5d}
    \newline

    The result of Part 7 imply that $[SO(3, \ \mathbb{R}), \ SO(3, \ \mathbb{R})] = SO(3, \ \mathbb{R})$.
    \newline

    \textbf{Part 5e}
    \newline

    It is easy to verify that $[h, \ h] = \{0\}$, so it follows that $[g, \ g] = [[h, \ h], \ [h, \ h]] = \{0\}$ as well.

    \section{Problem 5}

    \textbf{Part 1}

    \begin{prop}
      For some natural $k$ (or $k = 0$), $g^{(k)} \subset g_{(k)}$.
    \end{prop}

    \begin{proof}
      We proceed by induction. Clearly, this is true for the case of $k = 0$ and $k = 1$. We note that for $k \geq 1$:

      $$g_{(k)} = [g_{(k - 1)}, \ g]$$

      and:

      $$g^{(k)} = [g^{(k - 1)}, \ g^{(k - 1)}]$$

      Assume that for the case of $k$, the proposition holds true. In the case of $k + 1$ we have $g_{(k + 1)} = [g_{(k)}, \ g]$ and
      $g^{(k + 1)} = [g^{(k)}, \ g^{(k)}]$. From the inductive hypothesis, $g_{(k)} \subset g_{(k)}$. Finally, we note that $g^{(k)}$ is a subalgebra of $g$, so $g^{(k)} \subset g$.
      \newline

      It follows from the definition of the bracket that:

      $$g^{(k + 1)} = [g^{(k)}, \ g^{(k)}] \subset [g_{(k)}, \ g^{(k)}] \subset [g_{(k)}, \ g] = g_{(k + 1)}$$

      and the proof by induction is complete.
      \end{proof}

    \begin{cor}
      Every nilpotent Lie algebra is solvable
    \end{cor}

    \begin{proof}
      Let $g$ be a nilpotent Lie algebra. It follows that there exists some $k$ such that $g_{(k)} = 0$. In the case that $k = 0$, this
      proposition clearly holds. Otherwise, there is some $k$ such that $[g_{(k - 1)}, \ g] = \{0\}$.
      \newline

      From above, we know that $[g^{(k - 1)}, \ g^{(k - 1)}] \subset [g_{(k - 1)}, \ g] = \{0\}$, so it follows that $g^{(k)} = 0$. By definition,
      the Lie algebra is solvable and the proof is complete.
    \end{proof}

    \textbf{Part 2}
    \newline

    Consider an Abelian Lie algebra $g$. Given two elements $x, \ y \in G$, we note that $[x, \ y] = 0$, so it follows that $g_{(1)} = [g, \ g] = 0$.
    Thus, every Abelian Lie algebra is nilpotent (and solvable).
    \newline

    \textbf{Part 3}
    \newline

    \begin{prop}
      Every $2$-dimensional Lie algebra is solvable.
    \end{prop}

    \begin{proof}
      Let $g$ be a two-dimensional Lie algebra. It follows that $g = \text{span}(v, \ w)$, for two linearly independent vectors $v$ and $w$.
      Given two elements in $g$, of the form $av + bw$ and $cv + dw$, we note that:

      $$[av + bw, \ cv + dw] = [av + bw, \ cv] + [av + bw, \ dw] = bc[w, \ v] + ad[v, \ w] = (ad - bc)[v, \ w] = z$$

      where $z \in g$. Therefore, $g^{(1)}$ has dimension less than or equal to $1$. It follows (from a previous question), that $g^{(1)}$ must be the Abelian Lie algebra. Therefore,
      $g^{(2)} = [g^{(1)}, \ g^{(1)}] = \{0\}$, so $g$ is solvable.
    \end{proof}

    \textbf{Part 4}
    \newline

    Consider the two-dimensional vector space $V$ with basis $v_1, \ v_2$. Consider the null-bracket, which combine with $V$ to form an Abelian Lie algebra,
    and the bracket such that $[v_1, \ v_2] = v_1$ and $[v_k, \ v_k] = 0$ (we then extend its action upon the basis to a bilinear map).
    \newline

    One can easily verify
    that the Jacobi identity holds for such a bracket.
    \newline

    \begin{prop}
      The two Lie algebras presented above are not isomorphic.
    \end{prop}

    \begin{proof}
      Assume that such an ismorphism $T$ from the Abelian Lie group to the second Lie group exists. We then must have:

      $$T([v_1, \ v_2]) = 0 = [T(v_1), \ T(v_2)] = [av_1 + bv_2, \ cv_1 + dv_2] = (ad - bc)[v_1, \ v_2] = (ad - bc)v_1$$

      Clearly, $v_1 \neq 0$, so we then must have $ad = bc$. Clearly, at least one of $c$ or $d$ must be non-zero, or else $T(v_1) = 0$, which would imply that $T$ is not an isomorphism.
      Therefore, in the case of $d \neq 0$:

      $$d T(v_1) = ad v_1 + db v_2 = bc v_1 + db v_2 = b(cv_1 + dv_2) = b T(v_2) \ \Rightarrow \ T(v_1) = \lambda T(v_2)$$

      which contradicts the fact that $T$ is an isomorphism. In addition, if $c \neq 0$, then:

      $$c T(v_1) = ac v_1 + bc v_2 = ac v_1 + ad v_2 = a(cv_1 + dv_2) = a T(v_2) \ \Rightarrow \ T(v_1) = \lambda' T(v_2)$$

      which again contradicts the fact that $T$ is an isomorphism. Therefore, we derive a contradiction in both cases. It follows that no such
      bijection exists and the proof is complete.
      \end{proof}

    \textbf{Part 5}
    \newline

    \begin{prop}
      Any two-dimensional Lie algebra is isomorphic to one of the two from Part 4.
    \end{prop}

    \begin{proof}
      Let $g$ be a two-dimensional Lie algebra. It follows that $g$ has a basis of the form $e_1, \ e_2$, along
      with a bracket. Clearly, such a bracket is completely determined by how it maps the pair $e_1, \ e_2$.
      \newline

      We will have $[e_1, \ e_2] = z$, where $z = c_1 e_1 + c_2 e_2$. In the case that $z$ is the zero vector, then
      we define an bijection that takes $v_1$ to $e_1$, and $v_2$ to $e_2$, and extend it to a linear map to form
      an ismorphism from $V$ to $g$.
      \newline

      We note that given $v, \ w \in V$, we will have:

      $$T([v, \ w]) = T(0) = 0 = [T(v), \ T(w)]$$

      so $T$ is a valid Lie algebra isomorphism.
      \newline

      Now, consider the case when $z$ is non-zero. We choose a vector $x$ such that $(z, \ x)$ is linearly independent. Noting that $[z, \ x] = \gamma z$,
      for some $\gamma \neq 0$ (or else $x$ and $z$ would be linearly dependent, as can be easily verified), we define a new basis for $g$ as $(z, \ x/\gamma)$.
      \newline

      We then
      define a bijection between bases that takes $v_1$ to $z$ and $v_2$ to $y = x/\gamma$, then extend it to a linear map (which is
      also an isomorphism).
      \newline

      We note that, given $v, \ w \in V$:

      $$T([v, \ w]) = T([av_1 + bv_2, \ cv_1 + dv_2]) = (ad - bc)T(v_1) = (ad - bc)z = \frac{1}{\gamma}(ad - bc)[z, \ x]$$
      $$= (ad - bc)[z, \ y] = [az + by, \ cz + dy] = [T(v), \ T(w)]$$

      so it follows that $T$ is a valid Lie algebra isomorphism.

    \end{proof}

    \textbf{Part 6}
    \newline

    Clearly, $H(3, \ \mathbb{R})$ is solvable, as $g^{(2)} = [[H(3, \ \mathbb{R}), \ H(3, \ \mathbb{R})], \ [H(3, \ \mathbb{R}), \ H(3, \ \mathbb{R})]] = 0$.
    \newline

    Since we proved that every $2$-dimensional Lie algebra is solvable, then $SL(2, \ \mathbb{R})$ and $SL(2, \ \mathbb{C})$ are solvable.
    \newline

    $g = SO(3, \ \mathbb{R})$ is not solvable, as $[SO(3, \ \mathbb{R}), \ SO(3, \ \mathbb{R})] = SO(3, \ \mathbb{R})$, so it follows from the definition of $g^{(k)}$ that
    $g^{(k)} = SO(3, \ \mathbb{R})$ for all $k$.
    \newline

    Finally, we note that for the example in Part (e) $g^{(1)} = [g, \ g] = [[h, \ h], \ [h, \ h]] = 0$ (in the same fashion as the first example).

    \section{Problem 6}

    \textbf{Part 1}
    \newline

    \begin{prop}
      Every simple Lie algebra is semi-simple.
      \end{prop}

    \begin{proof}
      This is the trivial case, $g$ is the direct sum of itself, which is simple, so it is semisimple
    \end{proof}

    \textbf{Part 2}
    \newline

    \begin{prop}
      $g$ is semisimple if and only if $g$ has no non-zero Abelian ideals.
    \end{prop}

    \begin{proof}
      First, assume that $g$ is semisimple. It follows that $g = o \oplus k$, where $o$ and $k$ are simple. Assume that $g$ has a non-zero Abelian ideal $i$.
      Note that at least one of the intersections $o \cap i$ and $k \cap i$ must be non-empty.
      \newline

      Assume without loss of generality that it is $o \cap i$. Since $o$ and $i$ are both vector subspaces, so too will be $o \cap i$.
      \newline

      We then note that $o \cap i$ is also an Abelian Lie subalgebra under $[]_{o}$. As given $A, \ B \in i \cap o$:

      $$[(A, \ 0), \ (B, \ 0)] = ([A, \ B]_{o}, \ 0) = [A, \ B]_{o} = 0$$

      Finally, we note that $[o, \ o \cap i]_{o} \subset o$, by definition of the bracket, and by definition of the ideal:

      $$[o, \ o \cap i]_{o} \subset [o, \ i]_{o} \subset [o \oplus k, \ i] \subset i$$

      so $o \cap i$ is a non-zero Abelian ideal of $o$. In the case that $o \cap i = o$, then $o$ is Abelian and we have a contradiction to the simplicity
      of $o$. Otherwise, $o \cap i$ is a non-zero proper ideal of $o$, so we also have a contradiction to the simplicity of $o$.
      \newline

      Therefore, $g$ cannot have any non-zero Abelian ideals.
      \newline

      \textcolor{red}{I still need to prove the other direction}
    \end{proof}

    \textbf{Part 4}
    \newline

    We note that $[H(3, \ \mathbb{R}), \ H(3, \ \mathbb{R}_]$ is a non-zero Abelian ideal of $H(3, \ \mathbb{R})$. Therefore, this Lie algebra is not semisimple.
    \newline

    Clearly, neither $SL(2, \ \mathbb{R})$ or $SL(2, \ \mathbb{C})$ have any non-zero Abelian ideals (as $[g, \ h] = g$ for any non-zero subalgebra $h$, so neither has any proper ideals, and is not itself Abelian, so they have
    no Abelian ideals). Thus, both of
    these algebras are semisimple.
    \newline

    The same logic holds true for this group: $[g, \ h] = g$ for any non-zero subalgebra $h$, so $SO(3, \ \mathbb{R})$ has no proper ideals, and is itself not Abelina, so it has no
    non-zero Abelian ideals. Therefore, it is semisimple.
    \newline

    Finally, in Part (e), the resulting algebra is clearly itself Abelian, so it is not semisimple.

    \end{document}
